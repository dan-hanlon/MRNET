{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DAVID_CONROY_DANIEL_HANLON_MRNET_CHALLENGE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa6a4d47426849ac8ecb500d1f9fb361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4d82fec4dd8d4e3c95ddc4c1fa0c38bb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_95d2ac5ae9dd4958990a3de3ae22bc64",
              "IPY_MODEL_7a6b8f80064f4eb28ac6acc980710daa",
              "IPY_MODEL_743fcd41496b493da3e710ba0ab21efe"
            ]
          }
        },
        "4d82fec4dd8d4e3c95ddc4c1fa0c38bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95d2ac5ae9dd4958990a3de3ae22bc64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6306a75374d845b2b1735cc58a0af619",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e6438c075124f179c7bc393cd9d7cae"
          }
        },
        "7a6b8f80064f4eb28ac6acc980710daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c47425fa51994cf2bc47da9841c5b31e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46830571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46830571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b0750301b7534446aca734e51b60ba8e"
          }
        },
        "743fcd41496b493da3e710ba0ab21efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e3741c068dd8407790368efb9db1f892",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 90.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d02e1c7cde70472b8c71a36002255ccb"
          }
        },
        "6306a75374d845b2b1735cc58a0af619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e6438c075124f179c7bc393cd9d7cae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c47425fa51994cf2bc47da9841c5b31e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b0750301b7534446aca734e51b60ba8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3741c068dd8407790368efb9db1f892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d02e1c7cde70472b8c71a36002255ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4yYhkqVGSBE",
        "outputId": "3f7f8986-767c-4d22-d512-9925aceeae50"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EYvmDhsGa5e",
        "outputId": "e10a5e60-a617-42a3-cc96-72d079755981"
      },
      "source": [
        "!pip install -e git+https://github.com/ncullen93/torchsample.git#egg=torchsample\n",
        "!pip install visdom\n",
        "!pip install nibabel\n",
        "!pip install h5py  \n",
        "!pip install torchsample\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining torchsample from git+https://github.com/ncullen93/torchsample.git#egg=torchsample\n",
            "  Cloning https://github.com/ncullen93/torchsample.git to ./src/torchsample\n",
            "  Running command git clone -q https://github.com/ncullen93/torchsample.git /content/src/torchsample\n",
            "Installing collected packages: torchsample\n",
            "  Running setup.py develop for torchsample\n",
            "Successfully installed torchsample-0.1.3\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[K     |████████████████████████████████| 676 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from visdom) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom) (22.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom) (1.15.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom) (7.1.2)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.1-py2.py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2.10)\n",
            "Building wheels for collected packages: visdom, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=356da9e91b32d8554decec09f68f8e25985e178419700ab16f6e324f864b5a71\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5710 sha256=91e2261af72286a06cfcc6f9fe774fbb1833f782fefee22b5a00ffe2aaa5d8b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built visdom torchfile\n",
            "Installing collected packages: jsonpointer, websocket-client, torchfile, jsonpatch, visdom\n",
            "Successfully installed jsonpatch-1.32 jsonpointer-2.1 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-1.2.1\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from nibabel) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: torchsample in ./src/torchsample (0.1.3)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_tLbvsqHQOS"
      },
      "source": [
        "Import the following libraries to run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VoP4P0uGeFZ"
      },
      "source": [
        "#import all libraries\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import pandas as pd\n",
        "from torch.autograd import Variable\n",
        "from src.torchsample.torchsample.transforms import RandomRotate, RandomTranslate, RandomFlip, ToTensor, Compose, RandomAffine\n",
        "from torchvision import transforms\n",
        "from tensorboardX import SummaryWriter\n",
        "import math\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "from tqdm import tqdm_notebook\n",
        "from torch.utils import data\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import pdb\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTgepNCNHkp5"
      },
      "source": [
        "# **The first step is to set our parameters for the task at hand:**\n",
        "\n",
        "**task** - 'acl' or 'meniscus' \n",
        "\n",
        "**model** - 'resnet-18' or 'alexnet'\n",
        "\n",
        "**directory** - directory to your data \n",
        "\n",
        "**lr** - learning rate \n",
        "\n",
        "**num_epochs** - number of epochs \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLFT_DeXIRx3"
      },
      "source": [
        "directory ='./gdrive/MyDrive/data/' #directory to the dataset in google drive\n",
        "#model = 'resnet-18' #resnet-18 or alexnet \n",
        "task = 'acl' # The task - acl or meniscus \n",
        "lr = 1e-5 #learning rate\n",
        "num_epochs = 15 # number of epochs\n",
        "early_trigger = 10\n",
        "early_stop = 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G43QGiZ-Ha4p"
      },
      "source": [
        "**Defining the model**\n",
        "\n",
        "The next step is to define the model. As previously mentioned, the model can be trained using both Alexnet and resnet-18 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaGnkCzhIg-w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "fa6a4d47426849ac8ecb500d1f9fb361",
            "4d82fec4dd8d4e3c95ddc4c1fa0c38bb",
            "95d2ac5ae9dd4958990a3de3ae22bc64",
            "7a6b8f80064f4eb28ac6acc980710daa",
            "743fcd41496b493da3e710ba0ab21efe",
            "6306a75374d845b2b1735cc58a0af619",
            "8e6438c075124f179c7bc393cd9d7cae",
            "c47425fa51994cf2bc47da9841c5b31e",
            "b0750301b7534446aca734e51b60ba8e",
            "e3741c068dd8407790368efb9db1f892",
            "d02e1c7cde70472b8c71a36002255ccb"
          ]
        },
        "outputId": "2f881cfd-f100-45a3-cb6f-7628da78a80c"
      },
      "source": [
        "models.resnet18(pretrained=True)\n",
        "\n",
        "#modify the last fully connected layer to output (1) instead of (1000)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pretrained_model =  nn.Sequential(*list(models.resnet18(pretrained=True).children())[:-1]  )    # delete the last fc layer.\n",
        "        self.classifer = nn.Linear(512, 1)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # input size of x (1, s, 3, 256, 256) where s is the number of slices in one MRI\n",
        "        x = torch.squeeze(x, dim=0) #output size (s, 3, 256, 256)\n",
        "        x = self.pretrained_model(x) #output size (s, 512)\n",
        "        output = torch.max(x, 0, keepdim=True)[0] #output size (1, 512)\n",
        "        output = self.classifer(output.squeeze(2).squeeze(2)) #output size (1)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa6a4d47426849ac8ecb500d1f9fb361",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsmdHB5dJGGs"
      },
      "source": [
        "The next step is to create a dataloader. The dataloader class is utilised to load the correct classes by taking the the *directory*, *task*, *plane*, *train* and *transform* as input. \n",
        "\n",
        "***train*** - *True or False* depending on the task. True is used for loading data and False is used for validating data \n",
        "\n",
        "**transform** - a compose function for performing transformations to the images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bihMO_eNJ475"
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, root_dir, task, plane, train=False, transform=None):\n",
        "        super().__init__()\n",
        "        self.task = task\n",
        "        self.plane = plane\n",
        "        self.root_dir = root_dir\n",
        "        self.train=train\n",
        "        if self.train == True:\n",
        "            self.folder_path = self.root_dir + 'train/{0}/'.format(plane)\n",
        "            self.records = pd.read_csv(\n",
        "                self.root_dir + 'train-{0}.csv'.format(task), header=None, names=['id', 'label'])\n",
        "        else:\n",
        "            self.folder_path = self.root_dir + 'valid/{0}/'.format(plane)\n",
        "\n",
        "            self.records = pd.read_csv(\n",
        "                self.root_dir + 'valid-{0}.csv'.format(task), header=None, names=['id', 'label'])\n",
        "\n",
        "        self.records['id'] = self.records['id'].map(\n",
        "            lambda i: '0' * (4 - len(str(i))) + str(i))\n",
        "        self.paths = [self.folder_path + filename +\n",
        "                      '.npy' for filename in self.records['id'].tolist()]\n",
        "        self.labels = self.records['label'].tolist()\n",
        "\n",
        "        self.transform = transform\n",
        "        \n",
        "        pos = np.sum(self.labels)\n",
        "        neg = len(self.labels) - pos\n",
        "        self.weights = [1, neg / pos]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        array = np.load(self.paths[index]) #load MRI \n",
        "        label = self.labels[index] #get label of MRI\n",
        "        label = torch.FloatTensor([label]) #convert type from numpy to torch\n",
        "\n",
        "        if self.transform: #if you are transforming it\n",
        "            array = self.transform(array) #transform the image\n",
        "            array = array.numpy()\n",
        "\n",
        "\n",
        "        array = np.stack((array,)*3, axis=1) #the model expects dimensions of (3, 256, 256), the MRIs are greyscale of size (256, 256). Therefore, we stack the image three times to fit the dimensions for the model.\n",
        "        array = torch.FloatTensor(array)\n",
        "\n",
        "        if label.item() == 1:\n",
        "            weight = np.array([self.weights[1]])\n",
        "            weight = torch.FloatTensor(weight)\n",
        "        else:\n",
        "            weight = np.array([self.weights[0]])\n",
        "            weight = torch.FloatTensor(weight)\n",
        "\n",
        "        return array, label, weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmmjCprOKFwd"
      },
      "source": [
        "**TRAINING THE MODEL**\n",
        "\n",
        "Initialise the model, optimiser, scheduler, transformation and data loader. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MkI7lfWKQiB"
      },
      "source": [
        "model = Net() #initialise the model\n",
        "if torch.cuda.is_available(): #if there is a GPU available, put the model on the GPU\n",
        "    model = model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= lr, weight_decay=0.1) #define the optimiser as Adam\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, patience=4, factor=.3, threshold=1e-4, verbose=True) #define a scheduler that decreases the learning rate if there has been no reduction in validation loss is four epochs\n",
        "\n",
        "#define a compose function that is a series of transformations on the images. \n",
        "augmentor = Compose([ \n",
        "            transforms.Lambda(lambda x: torch.Tensor(x)), #converts from numpy to tensor\n",
        "            RandomRotate(25), #rotate the image by 25 degrees\n",
        "            RandomTranslate([0.11, 0.11]), #blur the edges\n",
        "            RandomFlip(), #flip the image\n",
        "            ])\n",
        "\n",
        "\n",
        "#initialise the train and validation datasets (class we defined earlier) and then initialise a Pytorch's dataloader\n",
        "train_dataset_sag = Dataset(directory, task, \"sagittal\",train=True, transform=augmentor)\n",
        "train_dataset_axial = Dataset(directory, task, \"axial\",train=True, transform=augmentor)\n",
        "train_dataset_coronal = Dataset(directory, task, \"coronal\",train=True, transform=augmentor)\n",
        "\n",
        "valid_dataset_sag = Dataset(directory, task, \"sagittal\", train=False, transform = None)\n",
        "valid_dataset_axial = Dataset(directory, task, \"axial\", train=False, transform = None)\n",
        "valid_dataset_coronal = Dataset(directory, task, \"coronal\", train=False, transform = None)\n",
        "\n",
        "train_loader_sag = torch.utils.data.DataLoader(train_dataset_sag, batch_size=1, shuffle=True, num_workers=2, drop_last=False)\n",
        "train_loader_axial = torch.utils.data.DataLoader(train_dataset_axial, batch_size=1, shuffle=True, num_workers=2, drop_last=False)\n",
        "train_loader_coronal = torch.utils.data.DataLoader(train_dataset_coronal, batch_size=1, shuffle=True, num_workers=2, drop_last=False)\n",
        "\n",
        "valid_loader_sag = torch.utils.data.DataLoader(valid_dataset_sag, batch_size=1, shuffle=-True, num_workers=2, drop_last=False)\n",
        "valid_loader_axial = torch.utils.data.DataLoader(valid_dataset_axial, batch_size=1, shuffle=-True, num_workers=2, drop_last=False)\n",
        "valid_loader_coronal = torch.utils.data.DataLoader(valid_dataset_coronal, batch_size=1, shuffle=-True, num_workers=2, drop_last=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMICnI_aM6hR"
      },
      "source": [
        "## **Train model function**\n",
        "\n",
        "# **Input parameters:**\n",
        "\n",
        "**model** - initialised model \n",
        "\n",
        "**lr** - learned rate \n",
        "\n",
        "**train_loader** - train_loader function for specific plane \n",
        "\n",
        "**valid_loader** - valid loader function for specific plane \n",
        "\n",
        "**optimizer** - optimiser \n",
        "\n",
        "**task** - acl or meniscus \n",
        "\n",
        "**plane** - plane \n",
        "\n",
        "### **RETURNS**\n",
        "\n",
        "Saves the best performing model to drive based on AUC\n",
        "- Removes the previous best performing model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is8BKLYGMriU"
      },
      "source": [
        "try:\n",
        "    os.makedirs('./models/')\n",
        "except:\n",
        "  print('Directory already made')\n",
        "\n",
        "def train_model(model, lr, train_loader, valid_loader, num_epochs, optimizer, task, plane): \n",
        "  best_val_auc = 0 \n",
        "\n",
        "  for epoch in range(15):\n",
        "        current_lr = lr\n",
        "\n",
        "        y_preds = []\n",
        "        y_trues = []\n",
        "        losses = []\n",
        "        _ = model.train()\n",
        "        #loop through each MRI in the training set\n",
        "        for i, (image, label, weight) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #load all data onto the GPU\n",
        "            if torch.cuda.is_available():\n",
        "                image = image.cuda()\n",
        "                label = label.cuda()\n",
        "                weight = weight.cuda()\n",
        "\n",
        "            label = label[0]\n",
        "            weight = weight[0]\n",
        "\n",
        "            #pass the MRI through the model\n",
        "            prediction = model.forward(image.float()).squeeze(0)\n",
        "\n",
        "            #calculate the loss\n",
        "            loss = torch.nn.BCEWithLogitsLoss(weight=weight)(prediction, label)\n",
        "            loss.backward() #back propagation\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_value = loss.item()\n",
        "            losses.append(loss_value)\n",
        "\n",
        "            probas = torch.sigmoid(prediction) #convert output of model (logits) to a value between zero and one. This can be interpretted as a probability\n",
        "\n",
        "            y_trues.append(int(label[0]))\n",
        "            y_preds.append(probas[0].item())\n",
        "\n",
        "            try:\n",
        "                auc = metrics.roc_auc_score(y_trues, y_preds)\n",
        "            except:\n",
        "                auc = 0.5\n",
        "\n",
        "            train_loss = np.round(np.mean(losses), 4)\n",
        "            train_auc = np.round(auc, 4)\n",
        "\n",
        "        #evaluate the model on the validation data after each epoch\n",
        "        _ = model.eval()\n",
        "        y_trues = []\n",
        "        y_preds = []\n",
        "        losses = []\n",
        "        for i, (image, label, weight) in enumerate(valid_loader):\n",
        "\n",
        "          if torch.cuda.is_available():\n",
        "              image = image.cuda()\n",
        "              label = label.cuda()\n",
        "              weight = weight.cuda()\n",
        "\n",
        "          label = label[0]\n",
        "          weight = weight[0]\n",
        "\n",
        "          prediction = model.forward(image.float()).squeeze(0)\n",
        "\n",
        "          loss = torch.nn.BCEWithLogitsLoss(weight=weight)(prediction, label)\n",
        "\n",
        "          loss_value = loss.item()\n",
        "          losses.append(loss_value)\n",
        "\n",
        "          probas = torch.sigmoid(prediction)\n",
        "\n",
        "          y_trues.append(int(label[0]))\n",
        "          y_preds.append(probas[0].item())\n",
        "\n",
        "          try:\n",
        "              auc = metrics.roc_auc_score(y_trues, y_preds)\n",
        "          except:\n",
        "              auc = 0.5\n",
        "\n",
        "          val_loss = np.round(np.mean(losses), 4)\n",
        "          val_auc = np.round(auc, 4)\n",
        "\n",
        "        if val_auc > best_val_auc:\n",
        "          best_val_auc = val_auc\n",
        "          early_stop=0\n",
        "          for f in os.listdir('./models/'):\n",
        "                    if (task in f) and (plane in f):\n",
        "                        os.remove(f'./models/{f}')\n",
        "          file_name = f'model_{task}_{plane}_val_auc_{val_auc:0.4f}_train_auc_{train_auc:0.4f}_epoch_{epoch+1}.pth'\n",
        "          torch.save(model, f'./models/{file_name}')\n",
        "        else:\n",
        "          early_stop+= 1\n",
        "\n",
        "        if early_stop == early_trigger:\n",
        "          print('Early stopping after {} epochs'.format(epoch))\n",
        "          sys.exit()\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(\"epoch : {0} | train loss : {1} | train auc {2} | val loss {3} | val auc {4} \".format(\n",
        "            epoch, train_loss, train_auc, val_loss, val_auc))\n",
        "\n",
        "        \n",
        "        print('-' * 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jtt5r8wQOTR"
      },
      "source": [
        "train_model(model, lr, train_loader_sag, valid_loader_sag, num_epochs, optimizer, task, \"sagittal\")\n",
        "train_model(model, lr, train_loader_sag, valid_loader_sag, num_epochs, optimizer, task, \"axial\")\n",
        "train_model(model, lr, train_loader_sag, valid_loader_sag, num_epochs, optimizer, task, \"coronal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs_XklXJTDgU",
        "outputId": "6132adf8-f618-4fcc-af1b-1a31fcf15068"
      },
      "source": [
        "train_model(model, lr, train_loader_sag, valid_loader_sag, num_epochs, optimizer, task, \"coronal\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch : 0 | train loss : 1.1379 | train auc 0.5625 | val loss 0.7634 | val auc 0.5704 \n",
            "------------------------------\n",
            "epoch : 1 | train loss : 1.0686 | train auc 0.6789 | val loss 0.7567 | val auc 0.5962 \n",
            "------------------------------\n",
            "epoch : 2 | train loss : 0.9613 | train auc 0.7911 | val loss 0.6793 | val auc 0.7326 \n",
            "------------------------------\n",
            "epoch : 3 | train loss : 0.8531 | train auc 0.8329 | val loss 0.6236 | val auc 0.8272 \n",
            "------------------------------\n",
            "epoch : 4 | train loss : 0.7925 | train auc 0.8541 | val loss 0.5602 | val auc 0.8333 \n",
            "------------------------------\n",
            "epoch : 5 | train loss : 0.6975 | train auc 0.8912 | val loss 0.6201 | val auc 0.8903 \n",
            "------------------------------\n",
            "epoch : 6 | train loss : 0.5875 | train auc 0.926 | val loss 0.5197 | val auc 0.9127 \n",
            "------------------------------\n",
            "epoch : 7 | train loss : 0.5071 | train auc 0.9458 | val loss 0.8139 | val auc 0.9209 \n",
            "------------------------------\n",
            "epoch : 8 | train loss : 0.4858 | train auc 0.9481 | val loss 0.7336 | val auc 0.9074 \n",
            "------------------------------\n",
            "epoch : 9 | train loss : 0.4113 | train auc 0.9636 | val loss 0.4392 | val auc 0.9116 \n",
            "------------------------------\n",
            "epoch : 10 | train loss : 0.3898 | train auc 0.9671 | val loss 0.4668 | val auc 0.9127 \n",
            "------------------------------\n",
            "epoch : 11 | train loss : 0.3138 | train auc 0.9799 | val loss 0.6994 | val auc 0.9231 \n",
            "------------------------------\n",
            "epoch : 12 | train loss : 0.3723 | train auc 0.9688 | val loss 0.9453 | val auc 0.9212 \n",
            "------------------------------\n",
            "epoch : 13 | train loss : 0.2771 | train auc 0.9854 | val loss 0.5131 | val auc 0.9301 \n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "issFZcKCQmbg"
      },
      "source": [
        "# **Extract the predictions for logistic regression function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRkS6mdmQZ6H"
      },
      "source": [
        "ref - https://towardsdatascience.com/mrnet-competition-part-1-65fcfb1cfa5f\n",
        "\n",
        "def logistic_regression_prep(task, plane, train=True):\n",
        "    assert task in ['acl', 'meniscus', 'abnormal']\n",
        "    assert plane in ['axial', 'coronal', 'sagittal']\n",
        "    \n",
        "    models = os.listdir('/content/models')\n",
        "\n",
        "    model_name = list(filter(lambda name: task in name and plane in name, models))[0]\n",
        "    model_path = f'/content/models/{model_name}'\n",
        "\n",
        "    model_trained = torch.load(model_path)\n",
        "    _ = model_trained.eval()\n",
        "    \n",
        "    train_dataset = Dataset('./gdrive/MyDrive/data/', \n",
        "                              task, \n",
        "                              plane, \n",
        "                              train=True)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=1, shuffle=False, num_workers=2, drop_last=False)\n",
        "    \n",
        "    \n",
        "    \n",
        "    predictions = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for image, label, _ in train_loader:\n",
        "            logit = model_trained(image.cuda())\n",
        "            prediction = torch.sigmoid(logit)\n",
        "            predictions.append(prediction.item())\n",
        "            labels.append(label.item())\n",
        "\n",
        "    return predictions, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyPyxt6jQxAy"
      },
      "source": [
        "results = {}\n",
        "\n",
        "for plane in ['axial', 'coronal', 'sagittal']:\n",
        "    predictions, labels = logistic_regression_prep(task, plane, train = True)\n",
        "    results['labels'] = labels\n",
        "    results[plane] = predictions\n",
        "    \n",
        "X = np.zeros((len(predictions), 3))\n",
        "X[:, 0] = results['axial']\n",
        "X[:, 1] = results['coronal']\n",
        "X[:, 2] = results['sagittal']\n",
        "\n",
        "y = np.array(labels)\n",
        "\n",
        "logreg = LogisticRegression(solver='lbfgs')\n",
        "logreg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr78atWVRguI"
      },
      "source": [
        "results_val = {}\n",
        "\n",
        "for plane in ['axial', 'coronal', 'sagittal']:\n",
        "    predictions, labels = logistic_regression_prep(task, plane, train=False)\n",
        "    results_val['labels'] = labels\n",
        "    results_val[plane] = predictions\n",
        "\n",
        "predictions\n",
        "X_val = np.zeros((len(predictions), 3))\n",
        "X_val[:, 0] = results_val['axial']\n",
        "X_val[:, 1] = results_val['coronal']\n",
        "X_val[:, 2] = results_val['sagittal']\n",
        "\n",
        "y_pred = logreg.predict_proba(X_val)[:, 1]\n",
        "metrics.roc_auc_score(results_val['labels'], y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgZPoLIvTDWM"
      },
      "source": [
        "**GRAD-CAM**\n",
        "\n",
        "Select models which you would like to use to produce heatmaps from GRAD-CAMS \n",
        "\n",
        "model_sag = model on saggital plane\n",
        "\n",
        "model_axial - model on axial plane \n",
        "\n",
        "model_coronal - model on coronal plane "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "622ICesITa4E"
      },
      "source": [
        "model_sag = torch.load('/content/models/model_meniscus_axial_val_auc_0.6751_train_auc_0.9944_epoch_15.pth',map_location=torch.device('cpu'))\n",
        "model_axial = torch.load('/content/models/model_meniscus_axial_val_auc_0.6751_train_auc_0.9944_epoch_15.pth',map_location=torch.device('cpu'))\n",
        "model_coronal = torch.load('/content/models/model_meniscus_coronal_val_auc_0.6612_train_auc_0.8449_epoch_10.pth',map_location=torch.device('cpu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viT2EO_TTHFA"
      },
      "source": [
        "# forward hook\n",
        "features_blobs = []\n",
        "def hook_feature(module, input, output):\n",
        "    features_blobs.append(output.data.cpu().numpy())\n",
        "\n",
        "model_sag._modules.get('pretrained_model')[7].register_forward_hook(hook_feature);\n",
        "model_axial._modules.get('pretrained_model')[7].register_forward_hook(hook_feature);\n",
        "model_coronal._modules.get('pretrained_model')[7].register_forward_hook(hook_feature);\n",
        "\n",
        "#backward hook\n",
        "features_blobs_gr = []\n",
        "\n",
        "def backward_hook(module, grad_in, grad_out):\n",
        "    _ =features_blobs_gr.append(grad_out[0].cpu().numpy() )\n",
        "    \n",
        "model_sag._modules.get('pretrained_model')[7].register_backward_hook(backward_hook);\n",
        "model_axial._modules.get('pretrained_model')[7].register_backward_hook(backward_hook);\n",
        "model_coronal._modules.get('pretrained_model')[7].register_backward_hook(backward_hook);\n",
        "\n",
        "_=mod.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4lXYAFHU6SN"
      },
      "source": [
        "train_dataset = Dataset(directory, task,'coronal',transform=None)\n",
        "\n",
        "ind = list(range(0,1130))\n",
        "\n",
        "def gradcam(train_dataset, ind):\n",
        "  for i in range(0,len(ind)):\n",
        "      image,label,weight = train_dataset.__getitem__(ind[i])\n",
        "      if len(str(ind[i])) == 1:\n",
        "          string = '000' + str(ind[i]) \n",
        "      elif len(str(ind[i])) == 2:\n",
        "          string = '00' + str(ind[i]) \n",
        "      elif len(str(ind[i])) == 3:\n",
        "          string = '0' + str(ind[i]) \n",
        "      elif len(str(ind[i])) == 4:\n",
        "          string = str(i) \n",
        "\n",
        "      task = 'acl'\n",
        "      plane ='sagittal'\n",
        "      split = 'valid'\n",
        "\n",
        "      try:\n",
        "          os.makedirs('./cams7/'+ split +'/'  + plane +'/' + string)\n",
        "      except:\n",
        "          print('')\n",
        "      try:\n",
        "          os.makedirs('./heatmaps7/'+ split +'/' + plane +'/' + string)\n",
        "      except:\n",
        "          print('')\n",
        "      try:\n",
        "          os.makedirs('./img7/'+ split +'/' + plane +'/' + string)\n",
        "      except:\n",
        "          print('')\n",
        "      try:\n",
        "          os.makedirs('./cam_boundary7/'+ split +'/' + plane +'/' + string)\n",
        "      except:\n",
        "          print('')\n",
        "\n",
        "      \n",
        "      prediction = mod.forward(image.float())\n",
        "      probas = torch.sigmoid(prediction)\n",
        "      loss = torch.nn.BCEWithLogitsLoss(weight=weight)(prediction.squeeze(0), label)\n",
        "      loss.backward()\n",
        "      weights =nn.AdaptiveAvgPool2d(1)(torch.Tensor(features_blobs_gr[i]))\n",
        "      x = torch.Tensor(features_blobs[i])\n",
        "      gcam = torch.mul(x, weights).sum(dim=1, keepdim=True)\n",
        "      gcam = F.relu(gcam)\n",
        "      gcam = F.interpolate(gcam, (256,256), mode=\"bilinear\", align_corners=False)\n",
        "      B, C, H, W = gcam.shape\n",
        "\n",
        "\n",
        "      gcam = gcam.view(B, -1)\n",
        "      gcam -= gcam.min(dim=1, keepdim=True)[0]\n",
        "      gcam /= gcam.max(dim=1, keepdim=True)[0]\n",
        "      gcam = gcam.view(B, H, W)\n",
        "\n",
        "\n",
        "      for j in range(0, image.shape[0]):\n",
        "          img = image[j].numpy()\n",
        "          img = img.transpose(1, 2, 0)\n",
        "          heatmap = np.uint8(255 * gcam.numpy()[j])\n",
        "\n",
        "          heatmap = (cv2\n",
        "                      .cvtColor(cv2.applyColorMap(\n",
        "                          cv2.resize(heatmap, (256, 256)),\n",
        "                          cv2.COLORMAP_JET), \n",
        "                                cv2.COLOR_BGR2RGB)\n",
        "                    )\n",
        "          \n",
        "          \n",
        "          #Heatmap \n",
        "          heatmap1 = heatmap.copy()\n",
        "          \n",
        "          #Split images into respective colour channels \n",
        "          blue_img, green_img, red_img = cv2.split(heatmap)\n",
        "\n",
        "          # Focus on red \n",
        "          red_img = cv2.bitwise_not(red_img)\n",
        "          \n",
        "          # Perform a threshold on the img, removing all but red (changed to black iwth bitwise_not previously)\n",
        "          thresh = cv2.threshold(red_img, 254, 255, cv2.THRESH_BINARY)[1]\n",
        "          cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "          cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "          for c in cnts:\n",
        "            #Retrireve coordinates for rectange \n",
        "            x,y,w,h = cv2.boundingRect(c)\n",
        "            \n",
        "            # Place the bounding box on the heatmap\n",
        "            cv2.rectangle(heatmap, (x, y), (x + w, y + h), (36,255,12), 2)\n",
        "            #cv2.imwrite(\"test.png\",img)\n",
        "\n",
        "          # result - no changes \n",
        "          result =  img * 0.5\n",
        "\n",
        "          # cam - Heatmap + MRI slice \n",
        "          cam = img * 0.5 + heatmap1 * 0.5\n",
        "\n",
        "          # cam_boundary - Heatmap + MRI Slice + Bounding box \n",
        "          cam_boundary = img * 0.5 + heatmap * 0.5\n",
        "\n",
        "\n",
        "          # Output all files to respective folders \n",
        "          pil_img_result = Image.fromarray(np.uint8(result))\n",
        "          pil_img_cam = Image.fromarray(np.uint8(cam))\n",
        "          pil_cam_boundary = Image.fromarray(np.uint8(cam_boundary))\n",
        "          img_acc[string + \"/\" + str(j)] =  probas\n",
        "\n",
        "          pil_img_cam.save('./cams7/'+split + '/' +  plane +'/' + string +'/' +str(j) + '.png')\n",
        "          pil_img_result.save('./img7/'+split + '/' +  plane +'/' + string +'/' +str(j) + '.png')\n",
        "          pil_cam_boundary.save('./cam_boundary7/'+split + '/' +  plane +'/' + string +'/' +str(j) + '.png')\n",
        "          np.save('./heatmaps7/'+split + '/' +  plane +'/' + string +'/' +str(j) + '.npy', heatmap1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozIH96roWMhC"
      },
      "source": [
        "grad_cam(train_dataset_sag)\n",
        "grad_cam(train_dataset_axial)\n",
        "grad_cam(train_dataset_coronal)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}